---
title: 'Chapter 04: Classification'
author: "Stan Piotrowski"
date: "12/16/2021"
output:
  pdf_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r load libraries and source scripts}
pacman::p_load(tidyverse, ISLR2, patchwork, ggcorrplot, MASS, e1071, class)
source("../plot_themes.R")
```

## Notes

### Introduction

* In general, it's difficult to use ordinary least squares with qualitative variables that have more than 2 categories because they may not have a natural ordering.
* In addition, it's not advisable to perform a least squares regression for qualitative variables because the estimates of the average effect on the response may not be directly interpretable as a probability, unlike classification methods.

### Logistic regression

* Logistic regression models the probability of a class of qualitative variable given the predictor(s).
* The key difference between the linear regression and logistic regression model is the latter gives output-- the probability of a class of qualitative variable given the data-- that is bounded between 0 and 1.
* Importantly, when we're using estimated coefficients from the logistic regression to predict the probability of an outcome for different values of the predictor(s), we use the logistic function and use $e$ raised to the power of the coefficient estimates.
* Let's reproduce the figure from the textbook comparing the results from a model predicting `default` status from `balance` using ordinary least squares and logistic regression.

```{r compare ols and logistic regressions}
# Fit models to default data set
Default$dummy_default <- ifelse(Default$default == "Yes", 1, 0)
ols_mod <- lm(dummy_default ~ balance, data = Default)
logistic_mod <- glm(default ~ balance, data = Default, family = "binomial")

# Plot
p1 <- ggplot(aes(balance, dummy_default), data = Default) + 
  geom_point(alpha = 0.5) + 
  geom_abline(intercept = ols_mod$coefficients[1], slope = ols_mod$coefficients[2], 
              color = "blue") + 
  scale_x_continuous(labels = scales::label_dollar()) +
  labs(x = "Balance", y = "Default status", title = "Ordinary least squares regression") + 
  plot_theme

# Add predicted probabilities for each observation
Default$prob <- predict(logistic_mod, type = "response")

p2 <- Default %>% 
  ggplot(aes(balance, dummy_default)) + 
  geom_point() + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              se = FALSE) + 
  scale_x_continuous(labels = scales::label_dollar()) +
  labs(x = "Balance", y = "Default status", title = "Logistic regression") + 
  plot_theme

# Put the plots together
wrap_plots(p1, p2)
```

 * It's important to consider how our interpretation of the effects of predictors on the probability of response when dealing with single predictors in a logistic regression vs. including multiple predictors.  This is due to confounding, or the fact that in some cases, some predictors are correlated with one another, but may not be included in a model with a single predictor or a subset of predictors.  A good example of this is the interpretation of `student` status in the `default` data set-- when evaluated on its own, `student` has a positive probability on default; but when included in a multiple logistic regression model, `student` has a negative effect on default.
* In order to predict the probability of default for a given set of predictor values, we can either plug them in manually to the logistic equation, or use the `predict()` function with a data frame of new values.

```{r predict new values}
logistic_mod2 <- glm(dummy_default ~ student + income + balance, data = Default, family = "binomial")
summary(logistic_mod2)

# Predict probability of default for student
predict(logistic_mod2, newdata = data.frame(student = "Yes", balance = 1500, income = 40000), type = "response") # 0.05788

# Predict probability of default for non-student
predict(logistic_mod2, newdata = data.frame(student = "No", balance = 1500, income = 40000), type = "response") # 0.1049919
```

* For cases with more than 2 classes of response variable, we can extend logistic regression to the multinomial space, re-writing the generic logistic function to include $\beta_{k1}X_1 + ... + \beta_{kp}X_p$, where $K$ is the number of classes of the response.
* The only thing we need to do at the start is to decide which level should be treated as baseline-- however, although the coefficient estimates will differ, the log odds between any pair of classes and the fitted values will remain the same regardless of which class is chosen as the baseline.

### Linear discriminant analysis

* There are a few reasons why you wouldn't want to use logistic regression-- for example, if the difference is substantial between two classes of response, logistic regression estimates can be very unstable.
* We then estimate the density function of a particular observation coming from a given class of the response to calculate the posterior probability-- or the probability that the observation belongs to one of $K$ classes of response given the predictor value.
* One important assumption of these classifier is that we assume the functional form is normal or Gaussian.  Let's plot two normal distributions.  

```{r visualize two random Gaussian distributions}
# Construct vectors from normal distributions with the same variance and different means
data.frame(norm1 = rnorm(1e5, mean = -1.25, sd = 1), 
           norm2 = rnorm(1e5, mean = 1.25, sd = 1)) %>% 
  pivot_longer(cols = everything(), names_to = "norm_type", values_to = "value") %>% 
  ggplot(aes(value, fill = norm_type, color = norm_type)) + 
  geom_density(alpha = 0.7) + 
  geom_vline(xintercept = c(-1.25, 1.25), linetype = "dashed") + 
  labs(x = "Gaussian random variable", y = "Density", fill = "") + 
  theme(legend.position = "none")
```

* We can plug in the density function for a Gaussian distribution into Bayes theorem, then do some rearranging to get the function $\delta_k(x) = x * (\mu_k / \sigma^2) - (\mu^2_k / 2\sigma^2) + log(\pi_k)$
* In linear discriminant analysis, we assign an observation to the $k$ class with the highest $\delta$ estimate-- in the textbook, they simplify this in a 2 class case by assuming that both distributions have the same variance (0.5) and the same prior probability (0.5).
* The linear discriminant analysis estimates the key parameters using $\mu_k$, an estimate of the average, the mean of all the training observations in a particular $k$ class; $\sigma_2$, or the sample variances for each of $k$ classes, weighted by the sample size; and $\pi_k$, or the prior probability of an observation belonging to a particular class (this is just the proportion of observations for each of $k$ classes in the training data).
* To explore how the linear discriminant analysis method works, let's take some random samples from each of the normal distributions, calculate the key values, and assign classes to each, then calculate the prediction accuracy.

```{r manual linear discriminant analysis}
# Build data frame with two normal distributions with the same variance but different means
norm_df <- data.frame(norm1 = rnorm(1e5, mean = 1.25, sd = 0.5), 
                      norm2 = rnorm(1e5, mean = -1.25, sd = 0.5))

# Sample 
norm1_sample <- sample(norm_df$norm1, 50)
norm2_sample <- sample(norm_df$norm2, 50)

# Estimate mean, variance for each 
mean_norm1 <- mean(norm1_sample)
mean_norm2 <- mean(norm2_sample)

var_norm1 <- var(norm1_sample)
var_norm2 <- var(norm2_sample)

# Create function to calculate delta
calc_delta <- function(mu, variance, obs) {
  
  stat <- obs * (mu / variance) - (mu^2 / 2*variance)
  return(stat)
}

# Test the function on a value from the first normal distribution sample
calc_delta(mean_norm1, var_norm1, 0.78) # 3.73 -- this would be assigned to class 1
calc_delta(mean_norm2, var_norm2, 0.78) # -5.47

# Pit this together and write another function to assign the class
assign_class <- function(obs) {
  k <- calc_delta(mean_norm1, var_norm1, obs) > calc_delta(mean_norm2, var_norm2, obs)
  if (k == TRUE) {
    return("norm1")
  } else {
    return("norm2")
  }
}

# Calculate the decision boundary-- this is (mean norm1 + mean norm2) / 2
decision_boundary <- (mean(norm1_sample) + mean(norm2_sample)) / 2 # non-zero, but very close

# We know that the true decision boundary is 0, because we know what the estimate is for the first sample and the second sample

# Assign classes using the new function
res <- data.frame(norm1 = norm1_sample, norm2 = norm2_sample) %>% 
  pivot_longer(cols = everything(), names_to = "distribution", values_to = "value") %>% 
  mutate(assignment = map_chr(value, ~ assign_class(.x)), 
         check_assignment = distribution == assignment) %>% 
  count(check_assignment)

# All of them are TRUE except for one

# Plot distributions of the sampled observations and the decision boundary
data.frame(x = norm1_sample, y = norm2_sample) %>% 
  pivot_longer(cols = everything(), names_to = "type", values_to = "value") %>% 
  ggplot(aes(value, fill = type, color = type)) + 
  geom_density(alpha = 0.75) + 
  geom_vline(xintercept = c(-1.25, 1.25, decision_boundary), linetype = "dashed")

# Let's wrap this whole process up into a function, then repeatedly sample the normal distribution to calculate the error rates
lda_manual <- function(df) {
  
  # Sample from normal distribution
  norm1 <- sample(df$norm1, size = 500)
  norm2 <- sample(df$norm2, size = 500)
  
  # Calculate mean and variance
  mean_norm1 <- mean(norm1)
  mean_norm2 <- mean(norm2)
  
  var_norm1 <- var(norm1)
  var_norm2 <- var(norm2)
  
  res <- data.frame(norm1 = norm1, norm2 = norm2) %>% 
    pivot_longer(cols = everything(), names_to = "type", values_to = "value") %>% 
    mutate(assignment = map_chr(value, ~ assign_class(.x)), 
           check_assignment = type == assignment) %>% 
    count(check_assignment)
  
  false <- res$n[res$check_assignment == FALSE]
  total <- sum(res$n)
  error_rate <- round((false / total) * 100, 2)
  return(error_rate)
}

# Test function
lda_manual(norm_df)

# Repeat the function 100 times and plot
error_rates <- data.frame(rate = replicate(100, lda_manual(norm_df), simplify = TRUE))
ggplot(aes(rate), data = error_rates) +
  geom_density()
```

* In the case of applying an LDA classifier to data sets with more than 1 predictor, we make the assumption that each predictor follows a multivariate Gaussian distribution and allow some correlation between pairs of predictors.
* Importantly, for each predictor, we have a class-specific mean vector and a covariance matrix that's common for all of the $K$ classes of that predictor.
* The $\delta$ function is a linear function of the predictor $x$ and the decision rule is defined as a linear combination of $x$.
* Similar to the 1-dimensional case, LDA assigns the observation to the class with the highest $\delta$.
* To illustrate how this process works, we can predict `default` status based on `student` status and `balance`.
* Sensitivity-- the percentage of true positives that are identified by a classifier.
* It may be worth modifying the posterior probability threshold depending on the context of the problem.  Below, we'll re-create Figure 4.7 to visualize the overall error rate as a function of the posterior probability threshold.  Note-- come back to this when I figure out how they created this figure.

```{r recreate figure 4.7}
# Perform the LDA on the default data set
default_lda <- MASS::lda(default ~ student + balance + income, data = Default)
predictions <- data.frame(predict(default_lda, type = "response")) %>% 
  set_names(c("class", "posterior_no", "posterior_yes", "LD1"))

# Bind with original data frame
Default$posterior_no <- predictions$posterior_no
Default$posterior_yes <- predictions$posterior_yes

# Build function to calculate the error rates
# Build a confusion matrix based on the predictions, then tally the number of false positives and false negatives divided by the total number of observations
# Also take the prediction threshold as an argument

# Note, this needs to be fixed-- not matching the predicted values in the confusion matrix on page 150
calc_error_rate <- function(input, prediction_threshold) {

  out <- input %>% 
  dplyr::select(default, contains("posterior")) %>% 
  mutate(predicted_class = ifelse(posterior_no > prediction_threshold, "No", "Yes"),
         true_neg = ifelse(default == "No", 1, 0), 
         true_pos = ifelse(default == "Yes", 1, 0), 
         predicted_neg = ifelse(predicted_class == "No", 1, 0), 
         predicted_pos = ifelse(predicted_class == "Yes", 1, 0), 
         false_pos = ifelse(true_neg == 1 & predicted_pos == 1, 1, 0), 
         false_neg = ifelse(true_pos == 1 & predicted_pos == 0, 1, 0)) %>% 
  dplyr::select(-default, -contains("posterior"), -predicted_class) %>% 
  summarise(across(everything(), ~ sum(.)))
  
#summarise(error_rate = (sum(false_pos) + sum(false_neg)) / 10000)
  
  return(out)
}

calc_error_rate(Default, 0.2) %>% 
  dplyr::select(contains(c("true", "false", "predicted"))) %>%
  summarise(across(.cols = everything(), ~ sum(.)))

# Calculate the error rates for a range of prediction thresholds 
# The error rates are defined as the number of misclassifications-- false positives and false negatives-- divided by the total number of observations
confusion_mat_full <- map(
  seq(0, 0.5, 0.1), 
  ~ calc_error_rate(Default, .x) %>% 
    mutate(prediction_threshold = .x)) %>% 
  bind_rows()
  
# Calculate error rates
error_rates <- confusion_mat_full %>% 
  dplyr::select(false_pos, false_neg, prediction_threshold) %>% 
  mutate(misclassifications = false_pos + false_neg, 
         error_rate = misclassifications / 10000)

# Plot
error_rates %>% 
  ggplot(aes(prediction_threshold, error_rate)) + 
  geom_line(linetype = "dashed")
```

* Another helpful summary visualization is the ROC curve-- we can use it to display the false positive rate and the false negative rate for a given classifier and use the area under the curve (AUC) to evaluate the performance of the model.  Ideally, the ROC curve will hug the top left corner, with a high true positive rate (y-axis) and low false positive rate (x-axis).
* ROC curves display the sensitivity (true positive rate) against the specificity (false positive rate, 1 - sensitivity).

### Quadratic discriminant analysis

* Unlike LDA, which assumes that data in each class are normally distributed with a common covariance matrix for all classes, quadratic discriminant analysis (QDA) relaxes the common covariance assumption.
* Normally, $p(p+1)/2$ coefficients must be estimated for $p$ predictors, but if we assume a separate covariance matrix for each class like in QDA, we now have to estimate $Kp(p+1)/2$ coefficients.
* However, with LDA, we'd only have to estimate $Kp$ coefficients.  
* The choice of LDA vs QDA deals with the bias-variance trade-off-- in general, in situations where there are few training observations and thus the risk of variance between test sets is high, LDA would be preferable and we would sacrifice some bias; however, in other situations where we have more training observations and variance in the test data isn't a major concern, or the error introduced by assuming a common covariance matrix is too high, we'd choose the QDA.
* Let's try to explore the difference between LDA and QDA using the `iris` data set, using just `Sepal.Length` and `Seapal.Width`.  

```{r LDA and QDA comparison with iris}
# Let's just use two variables from the iris data
# First, let's confirm that the variables are normally distributed
p1 <- iris %>% 
  ggplot(aes(sample = Sepal.Length)) + 
  stat_qq(alpha = 0.7) + 
  stat_qq_line(linetype = "dashed") + 
  facet_wrap(~ Species, scales = "free") + 
  labs(x = "Theoretical", y = "Sample", title = "Sepal length")

p2 <- iris %>% 
  ggplot(aes(sample = Sepal.Width)) + 
  stat_qq(alpha = 0.7) + 
  stat_qq_line(linetype = "dashed") + 
  facet_wrap(~ Species, scales = "free") + 
  labs(x = "Theoretical", y = "Sample", title = "Sepal width")

patchwork::wrap_plots(p1, p2, nrow = 2)
```

* First after checking the normality assumptions, there appears to be some skew in `virginica`, although it's unclear how biased the results will be if the normality assumption is grossly violated.
* Now we should calculate the covariance matrix between both variables in all species.

```{r iris correlation}
iris %>% 
  group_by(Species) %>% 
  summarise(correlation = cor(Sepal.Length, Sepal.Width))
```

* From here, we can see that the correlation is not consistent across all classes of `Species`-- this would be a good opportunity to look at the difference between LDA and QDA to evaluate the effects of the common correlation structure across classes.  

### Naive Bayes

* In Bayes theorem, we can estimate the probability of a particular observation belonging to a given class using the prior probability, the density function of a predictor for an observation that comes from a specific class, and the product of the prior and density functions for all classes.  
* To estimate the density functions, we need estimates of the mean and covariance for a given class-- in LDA, we assume a class-specific mean and common covariance matrix; in QDA, we assume a class-specific mean and class-specific covariance matrix.
* In a naive Bayes setting, we assume that predictors in each class are independent.
* In general, the naive Bayes assumption of independence among predictors in each class is reasonable and the model performs well in situations where there are few training observations and the need to reduce variance is more prominent.

### Comparing between methods

* Ultimately, the choice of which model to use will depend on the structure of the data and the shape of the true decision boundary-- if the decision boundary is linear, LDA and logistic regression will generally be best; if slightly non-linear, QDA or naive Bayes may be good choices; and if the decision boundary is very complicated and requires a more flexible model, KNN will generally outperform the others.
* My general takeaway from this section is that for each data set, a substantial amount of exploratory data analysis is required to validate assumptions and guide the decision of which model to use.  Ultimately, though, it may mean using many different models and comparing the output (e.g., ROC curve).

### Generalized linear models

Starting in this next section, we're no longer considering a response variable that's qualitative (like default status using the loan data) or continuously quantitative.  Instead, we'll look at non-negative integers, or counts, using the `Bikeshare data`:

```{r bikeshare ols}
# Fit model with ordinary least squares
bikeshare_ols <- lm(bikers ~ workingday + temp + weathersit + mnth + hr,
                    data = Bikeshare)

summary_ols <- broom::tidy(summary(bikeshare_ols))
ols_p1 <- summary_ols %>% 
  filter(grepl("mnth", term)) %>% 
  mutate(term = factor(gsub("mnth", "", term), 
                       levels = c("Jan", "Feb", "March", "April", 
                                  "May", "June", "July", "Aug", "Sept", 
                                  "Oct", "Nov", "Dec")), 
         group = 1) %>% 
  ggplot(aes(term, estimate, group = group)) + 
  geom_path() +
  geom_point() +
  labs(x = "Month", y = "Coefficient", 
       title = "OLS") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ols_p2 <- summary_ols %>% 
  filter(grepl("hr", term)) %>% 
  mutate(term = as.numeric(gsub("hr", "", term)), 
         group = 1) %>% 
  ggplot(aes(term, estimate, group = group)) + 
  geom_path() + 
  geom_point() + 
  labs(x = "Hour", y = "Coefficient", 
       title = "OLS")

patchwork::wrap_plots(ols_p1, ols_p2)
```

It's worth looking at the summary of the model output.

```{r ols bikeshare summary}
head(broom::tidy(summary(bikeshare_ols)))
```

One of the most apparent issues with the results is that some of the coefficient estimates are negative-- this cannot be possible, because we can't have negative bikers.  Next, it's worth looking at the relationship between the mean and variance, since the ordinary least squares method assumes that the error term has a mean of zero and a constant variance, with no correlation with the covariates. 

```{r check constant variance}
# Plot the number of bikers per hour-- this should change by month
p1 <- ggplot(Bikeshare, aes(hr, bikers)) + 
  geom_boxplot() + 
  labs(x = "Hour", y = "Number of bikers")

# Plot the log-transformed data
p2 <- ggplot(Bikeshare, aes(hr, log(bikers))) + 
  geom_boxplot() + 
  labs(x = "Hour", y = "log(number of bikers)")

wrap_plots(p1, p2)
```

Using the log-transformation of the response appears to help the correlation between the covariates and the variance, but interpretation is challenging on this scale.  For thoroughness, it may also be worth looking at the fitted values against the studentized residuals.  We'll need to remove some of the inifinite studentized residuals.

```{r bikeshare fitted values and residuals}
# Create diagnostic data frames
ols_diagnostics <- data.frame(
  fit = fitted(bikeshare_ols), 
  resid = residuals(bikeshare_ols), 
  stud_resid = MASS::studres(bikeshare_ols),
  hat_values = hatvalues(bikeshare_ols)
)

ols_std_resid <- ols_diagnostics %>% 
  filter(across(everything(), ~ !is.na(.x))) %>% 
  ggplot(aes(fit, stud_resid)) + 
  geom_point(alpha = 1/5) +
  geom_hline(yintercept = c(0, -3, 3), linetype = "dashed", 
             color = "red") + 
  geom_smooth(se = FALSE) + 
  labs(x = "Fitted", y = "Studentized residuals", 
       title = "OLS")

ols_std_resid
```

We can also see from the plot of the studentized residuals above that the odinary least squares regression isn't a great fit for these data.

### Poisson regression

Instead of modeling the number of bikers as a function of the covariates using ordinary least squares, which assumes continuous responses, we can use Poisson regression.  This technique relies on the Poisson distribution, which describes the probability of observing counts over a defined interval.  Importantly, the Poisson assumes the mean and variance are equal, so we'll use the log-transformed number of bikers in the model.  Note, the code below to run the Poisson GLM is the same as the textbook, although the coefficient estimates are slightly different.  

```{r bikeshare poisson}
Bikeshare$log_bikers <- log(Bikeshare$bikers)
bikeshare_poisson <- glm(
  bikers ~ mnth + hr + workingday + temp + weathersit, 
  data = Bikeshare, 
  family = "poisson"
)

# Coefficients data frame
coefs <- data.frame(coefficients(bikeshare_poisson)) %>% 
  set_names("coef") %>% 
  rownames_to_column("term") %>% 
  filter(grepl("mnth", term) | grepl("hr", term))

months <- filter(coefs, grepl("mnth", term)) %>% 
  mutate(
    term = factor(
      gsub("mnth", "", term), 
      levels = c("Jan", "Feb", "March", "April", "May", "June", "July", "Aug", "Sept",
                 "Oct", "Nov", "Dec")
    )
  )

hrs <- filter(coefs, grepl("hr", term)) %>% 
  mutate( term = as.numeric(gsub("hr", "", term)))

poisson_p1 <- ggplot(months, aes(term, coef, group = 1)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Month", y = "Coefficient", title = "Poisson") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

poisson_p2 <- ggplot(hrs, aes(term, coef, group = 1)) + 
  geom_point() + 
  geom_line() +
  labs(x = "Month", y = "Coefficient", title = "Poisson")

# Compare OLS and Poisson coefficients
wrap_plots(ols_p1, ols_p2, poisson_p1, poisson_p2)
```

Let's see what the studentized residuals look like to evaluate model fit.

```{r poisson studentized residuals}
poisson_std_resid <- data.frame(
  fitted = fitted(bikeshare_poisson),
  std_resid = MASS::studres(bikeshare_poisson)
)

poisson_std_resid_plot <- ggplot(poisson_std_resid, aes(fitted, std_resid)) + 
  geom_point(alpha = 1/5) + 
  geom_hline(yintercept = c(-3, 0, 3), linetype = "dashed", color = "red") + 
  geom_smooth(se = FALSE) + 
  labs(x = "Fitted", y = "Studentized residuals", 
       title = "Poisson")

wrap_plots(ols_std_resid, poisson_std_resid_plot)
```

These plots demonstrate that in general, the Poisson model is a better fit relative to the OLS model.

### GLMs in greater generality

In general, all regression approaches model the mean of a response as a function of a set of predictors.  Importantly, for cases where the response is non-linear (e.g., binary or qualitative), a link function is used to transform the mean of the response as a linear function of the predictors (e.g., logit, as the log-link function).

## Lab

### Stock Market Data

We will use the `Smarket` data tabulates the daily percentage returns for the S&P 500 stocks between 2001 and 2005.  The data include `Lag` variables- the percentage returns for the previous 5 trading days.  Ultimately, we want to predict `Direction`, or whether the market will go up or down, based on the other features. These include the lag variables, as well as `Year`, `Volume` (shares traded in the previous day in billions), and `Today` (shares traded on that day).  First, we'll get an idea of how correlated the variables are to one another using a correlation plot.

```{r stock market correlation plot}
as.matrix(Smarket[, -9]) %>% 
  cor() %>% 
  ggcorrplot()
```

From this plot, similar to what was described in the book, there's really no correlation with any of the quantitative variables, with the exception of `Volume`.  

### Logistic regression

The goal in this section is to use logistic regression to predict the `Direction` using the quantitative variables.

```{r stock market logistic regression}
smarket_logistic_mod <- glm(
  formula = Direction ~ ., 
  data = select(Smarket, -c("Today", "Year")), 
  family = "binomial"
)

summary(smarket_logistic_mod)
```

None of the p-values here for any of the terms are statistically significant.  Still, we can use this model to predict probabilities and evaluate how well the model performs relative to the observed data.

```{r logistic confusion matrix}
# Compute probabilities using the model
logistic_probs <- predict(smarket_logistic_mod, type = "response")

# Make predictions based on probabilities
logistic_pred <- ifelse(logistic_probs > 0.5, "Up", "Down")

# Build the confusion matrix
# The columns represent the real data; the rows are the predictions
table(logistic_pred, Smarket$Direction)

# What proportion of the market was accurately predicted by the data?
mean(logistic_pred == Smarket$Direction) # 0.5216, or about 51% of the time
```

Now we'll want to train the data on a subset of observations, because in our initial analysis, we used the entire data set.  Let's subset our data to only include observations from before 2005.

```{r logistic training data}
# Split into training and test data
(training_data <- filter(Smarket, Year < 2005)) # 998 observations
(validation_data <- filter(Smarket, Year == 2005)) # 252 observations

# Fit logistic regression
logistic_training_mod <- glm(
  formula = Direction ~ ., 
  data = select(training_data, -c("Today", "Year")), 
  family = "binomial"
)

# Make new predictions with validation data
logistic_val_probs <- predict(
  logistic_training_mod,
  newdata = validation_data, 
  type = "response"
)

logistic_val_pred <- ifelse(logistic_val_probs > 0.5, "Up", "Down")
table(logistic_val_pred, validation_data$Direction)
(accuracy <- mean(logistic_val_pred == validation_data$Direction)) # 48% accurate
1 - accuracy # 52% error rate, a bit better-- this is the test error rate
```

Now we'll refit the logistic regression model with just the `Lag1` and `Lag2` variables, then see how well it performs using the validation data from 2005.

```{r logistic refit variables}
# Fit the model with the training data-- before 2005
logistic_training_mod <- glm(
  Direction ~ Lag1 + Lag2, 
  data = select(training_data, -c("Today", "Year")), 
  family = "binomial"
)

# Make new predictions with validation data
logistic_val_probs <- predict(
  logistic_training_mod,
  newdata = validation_data, 
  type = "response"
)

logistic_val_pred <- ifelse(logistic_val_probs > 0.5, "Up", "Down")
table(logistic_val_pred, validation_data$Direction)
accuracy <- mean(logistic_val_pred == validation_data$Direction) # 56% accurate
1 - accuracy # 44% error rate

```

### LDA

First, we'll fit the LDA model using the same training data before 2005 with just `Lag1` and `Lag2`.  

```{r LDA with lag1 and lag2}
# Fit model with the training data
(lda_mod1 <- lda(Direction ~ Lag1 + Lag2, data = training_data))

# Predict with 2005 data
(lda_predictions <- predict(lda_mod1, newdata = validation_data))

# Look at the class predicted by the model
(lda_class <- lda_predictions$class)

# How accurate was the test?
mean(lda_class == validation_data$Direction) # about 56% accuracy

```

Note, the above class predictions are based on a 50% probability threshold.  If we wanted to modify the posterior probability threshold for class assignment, we could manually do so.  For example, let's see how the prediction accuracy changes using different posterior probabilities.

```{r varying probability thresholds for prediction}
# Calculate accuracy for each threshold
accuracy <- map(
  seq(0.1, 0.9, 0.1), 
  ~ifelse(lda_predictions$posterior[, 1] > .x, "Down", "Up")
  ) %>% 
  map_dbl(~ mean(.x == validation_data$Direction))
  
# Build data frame and plot
data.frame(posterior = seq(0.1, 0.9, 0.1), 
           accuracy = accuracy) %>% 
  ggplot(aes(posterior, accuracy)) + 
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(0.1, 0.9, 0.1), 
                     labels = seq(0.1, 0.9, 0.1)) +
  labs(x = "Posterior probability", 
       y = "Accuracy")
```

Here, we can see that only when we use the 50% probability threshold to we see any change in the prediction accuracy with the validation data.  

### QDA

Now we're going to fit a QDA model to the same data with the same two variables.

```{r QDA with lag1 and lag2}
# Fit the model
(qda_mod1 <- qda(Direction ~ Lag1 + Lag2, data = training_data))

# Predict with new data
(qda_predictions <- predict(qda_mod1, newdata = validation_data))

# How accurate is the model?
mean(qda_predictions$class == validation_data$Direction) # about 60% accuracy

```

Interestingly, it looks like the quadratic relationship modeled by the QDA model is more accurate at predicting the stock market than the linear forms.

### Naive Bayes

Fit the same two variables to model the direction of the stock market using a naive Bayes classifier.

```{r naive bayes with lag1 and lag2}
# Fit the model
(naive_bayes_mod1 <- naiveBayes(Direction ~ Lag1 + Lag2, data = training_data))

# Predict and compute accuracy
naive_bayes_predictions <- predict(naive_bayes_mod1, newdata = validation_data)

mean(naive_bayes_predictions == validation_data$Direction) # almost 60%, very similar to the QDA model prediction accuracy
```

To generate posterior probabilities of observations belonging to a particular class, we need to supply a different argument with the `predict()` function.

```{r predict posteriors from naive Bayes}
(probs <- predict(naive_bayes_mod1, newdata = validation_data, type = "raw"))

```

### KNN

Now we'll fit a K-nearest neighbors classifier using the `class::knn()` function.  The input data required is slightly different than the functions used for other classifiers we examined previously.  The function is written in C, with the R wrapper `class::knn()` mostly doing data type checking before passing off the inputs.  We can see this using `getAnywhere("knn")`.

```{r prepare KNN model input}
# Matrix of predictors with the training data
training_mat <- as.matrix(dplyr::select(training_data, Lag1, Lag2))

# Matrix of predictors with the validation data
validation_mat <- as.matrix(dplyr::select(validation_data, Lag1, Lag2))

# Vector of class labels for training observations
training_labels <- training_data$Direction
```

The model will take each of these inputs, along with a value for K, or the number of nearest neighbors to consider in the classifier, and makes predictions in a single command.

```{r fit KNN model}
# Fit many different values for the number of nearest neighbors to consider
set.seed(1) # important, random number needed to break ties
knn_predictions <- map(
  .x = seq(1, 30, 1), 
  .f = ~ class::knn(
    train = training_mat, 
    test = validation_mat, 
    cl = training_labels, 
    k = .x
  )
)

# Evaluate the accuracy of each and plot
accuracy_df <- data.frame(
  k = seq(1, 30, 1), 
  accuracy = map_dbl(knn_predictions, ~ mean(.x == validation_data$Direction))
)

arrange(accuracy_df, desc(accuracy))

ggplot(accuracy_df, aes(k, accuracy)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks = seq(1, 30, 1))


```

The prediction accuracy varies quite a lot across the range of different values for K.  Let's see which values have the highest accuracy.

```{r rank accuracy}
arrange(accuracy_df, desc(accuracy))
```

Interestingly, K=3 and K=11 are tied for the highest prediction accuracy.  It's marginally better than using LDA, but not quite as good as QDA or naive Bayes.

To illustrate the power of using the KNN classifier, we'll apply the method to the `Caravan` data set, which contains nearly 6,000 customer records with sociodemographic and product ownership data.  In this instance, we'll look at `Purchase` as the response variable, or whether the customer purchased a caravan insurance policy.  There are only about 6% of records in the data for customers that purchased the policy.  One important aspect to consider about the KNN classifier is the scale of the data, as the classifier considers observations that are nearest to it.  So, we need to scale the data, which ensures that each variable has a mean of 0 and standard deviation of 1.  

```{r standardize caravan data}
# Scale
caravan_std <- scale(Caravan[, -86])

# Confirm that each variable has a mean of 0 and standard deviation of 1
check_std <- apply(caravan_std, 2, function(x)
  if(near(mean(x), 0) & near(var(x), 1)) {
    print(TRUE)
  } else {
    print(FALSE)
  }
)

all(check_std == TRUE) # TRUE
```

Now we'll fit the KNN, using the first 1,000 observations as the test set, and the remaining observations as the training set.

```{r fit KNN to std caravan data}
# Split into training and test set
test <- 1:1000
caravan_training <- caravan_std[-test, ] # train on everything except 1000 observations
caravan_validation <- caravan_std[test, ] # train on the first 1000 observations

# Classification labels for training set
training_labels <- Caravan$Purchase[-test]
validation_labels <- Caravan$Purchase[test]

# Fit KNN using different values of K
set.seed(1)
caravan_predictions <- map(
  .x = seq(1, 25, 1), 
  .f = ~ knn(
    train = caravan_training, 
    test = caravan_validation, 
    cl = training_labels, 
    k = .x
  )
)

# Calculate accuracy
accuracy_df <- data.frame(
  k = seq(1, 25, 1), 
  accuracy = map_dbl(caravan_predictions, ~ mean(.x == validation_labels))
)

# Which value of K has the highest accuracy?
arrange(accuracy_df, desc(accuracy)) 

# Plot
ggplot(accuracy_df, aes(k, accuracy)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks = seq(1, 25, 1)) + 
  labs(x = "K", y = "Accuracy", title = "Prediction accuracy of KNN classifier")
```

The highest prediction accuracy is achieved using K=9, or considering 9 nearest neighbors.  However, we still achieve > 92% using K=3 or greater.  But, this is a bit misleading, since there are so few `Yes` purchase records in the data set (recall, about 6%).  We can consider a realistic scenario where we are only interested in targeting people for insurance sales that are most likely to buy it.  Let's see how many `Yes` we accurately predicted using each level of K.  

```{r yes predictions}
# For each value of K, look at the accuracy of Yes records
names(caravan_predictions) <- seq(1, 25, 1)
(accuracy_yes_df <- map(
  caravan_predictions, 
  ~ data.frame(
    prediction = .x, 
    true_outcome = validation_labels)
) %>% 
    bind_rows(.id = "k") %>% 
    mutate(pred_yes = ifelse(prediction == "Yes", 1, 0), 
           both_yes = ifelse(prediction == "Yes" & true_outcome == "Yes", 1, 0), 
           k = as.numeric(k)) %>% 
    group_by(k) %>% 
    summarise(accuracy = sum(both_yes) / sum(pred_yes)) %>% 
    mutate(accuracy = replace_na(accuracy, 0)))

# Plot
ggplot(accuracy_yes_df, aes(k, accuracy)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "K", y = "Accuracy", 
       title = "Prediction accuracy for number of true positives", 
       subtitle = "At K=1, we accurately predict 12% of true positive; at K=9, 100%")

```

The numbers we computed above will be a bit different from those in the book because the authors set the seed each time they fit a KNN model with a different value for K; we wrapped all this in a call to `map()`.  However, in general, the numbers are quite comparable.  If we just consider random guessing, we would only be correct about 6% of the time (that is, the true number of `Yes` records in the data set).  So if we target just those customers that are most likely to purchase insurance, we're almost twice as accurate than random guessing, even using a KNN with K=1 (accuracy = 0.117).  If we go up to K=5, similar to the book, we're about 26.7% accurate.  If we go up to K=9, we can perfectly predict those customers that are likely to purchase the insurance.  To review, this exercise was saying that out of the number of predicted people that are likely to purchase insurance, using various KNN classifiers will accurately predict these about 10% up to 100% of the time.  

### Poisson regression

Now we'll fit a Poisson regression model to the `Bikeshare` data.  First, we'll fit ordinary least squares model to the data and comparing the predictions to those obtained from the Poisson model.

```{r OLS vs Poisson}
# Fit the OLS model
ols_mod <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
              data = Bikeshare)

# Predictions
ols_predictions <- predict(ols_mod)

# Fit Poisson model
poisson_mod <- glm(
  bikers ~ mnth + hr + workingday + temp + weathersit, 
  data = Bikeshare, 
  family = "poisson"
)

# Predictions
poisson_predictions <- predict(poisson_mod, type = "response")

# Build data frame with true outcomes and the predictions from both models and each covariate
Bikeshare$ols_predictions <- ols_predictions
Bikeshare$poisson_predictions <- poisson_predictions

# Plot predictions for OLS and Poisson models
ggplot(Bikeshare, aes(ols_predictions, poisson_predictions)) + 
  geom_jitter(alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1, 
              linetype = "dashed", color = "red") + 
  labs(x = "OLS predictions", 
       y = "Poisson predictions") + 
  facet_wrap(~mnth, scales = "free")

```


## Exercises

### Conceptual

1) Show that the logistic function representation and the logit representation for the logistic function are equivalent.

The logistic function is:

$$p(X) = \frac{e^{B_{0} + {B_{1}X}}}{1 + e^{B_{0} + {B_{1}X}}}$$
The logit representation of the logistic function is:

$$\frac{p(X)}{1 - p(X)} = e^{B_{0} + B_{1}X}$$

5) This question looks at the differences between LDA and QDA:

  a) If the Bayes decision boundary is linear, we expect LDA (linear discriminant analysis) to perform better on both the training data set and the test data set.  
  
  b) If the Bayes decision boundary is non-linear, we generally expect QDA to perform better on the 
  
  c) In situations where there are relatively few samples, LDA is generally a better choice that QDA.  The key difference between the two methods is that the former assumes that for a given number of classes, each observation is drawn from a normal distribution and all classes share a common covariance matrix.  The letter method also assumes observations in each class are drawn from a normal distribution, but assumes that each class has its own covariance matrix.  In cases where there are relatively few samples, the flexibility of the QDA model tends to result in high variance between successive model fits, while LDA tends to perform better.  In situations where there is a relatively large number of training observations, QDA may perform better, as the cost of variance isn't quite as high.
  
  d) This answer is similar to c).  In situations where the decision boundary is linear, LDA will perform better as it models the true decision boundary.  This would be particularly evident if the number of training observations was relatively small, as QDA would tend to overfit on the training set, leading to high variance and larger test error rates.
  
6) Using the example coefficients for students in a statistics class.

  a) Estimating the probability that a student who studies for 40 hours and has a GPA of 3.5 gets an A in the class.  Using the logistic equation below:
  
  $$p(X) = \frac{e^{B_{0} + {B_{1}X}}}{1 + e^{B_{0} + {B_{1}X}}}$$
  $$p(A) = \frac{e^{-6 + (0.05 * 40) + (1*3.5)}}{1 + e^{-6 + (0.05 * 40) + (1*3.5)}}$$
  $$p(A) = \frac{0.6065307}{1.6065307}$$
  $$p(A) = 0.3775407$$
  Using the model coefficients we were given, the student only has a 38% chance of getting an A in the class.  
  
  b) How many hours would the student need to study in order to have a 50% chance of getting an A in the class?
  
```{r conceptual 6b}
calc_prob_a <- function(hours) {
  
  # Calculate logistic function
  p <- exp(-6 + (0.05*hours) + (1*3.5)) / (1 + exp(-6 + (0.05*hours) + (1*3.5)))
  return(p)
}

data.frame(
  hours = seq(40, 80, 1), 
  prob_a = map_dbl(seq(40, 80, 1), ~ calc_prob_a(.x))
) %>% 
  filter(prob_a == 0.5)
```
  
  Using the model output above, the student would need to study for 50 hours to even have a 50% chance of getting an A in the class. 
  
  7) We're asked to predict whether a stock will issue a dividend this year (yes or no) based on some predictor, X.  We know that for each class, "yes" or "no," the mean of the predictors is 10 and 0, respectively.  We also know they have a common variance of 36 and 80% of companies issued dividends.  We're asked to find the probability that a company will issue a dividend this year based on the predictor value of X = 4.
  
  To solve this problem, we can use the following equation:
  
$$Pr(Y = k|X = x) = \frac{\pi_kf_k(x)}{\sum^{K}_{l=1}\pi_lf_l(x)}$$
  
  We also know that $f_k(X)$, the density function, is that of a normal distribution, which is described by the equation:
  
$$f_k(x) = \frac{1}{\sqrt{2\pi\sigma_k}}exp(-\frac{1}{2\sigma^2_k}(x = \mu_k)^2)$$
  We can use the `dnorm()` function to compute the density given the value of `X`, the mean, and the standard deviation, then plug these into Bayes theorem above.  
  
```{r conceptual 7}
# Calculate probability
x <- 0.8 * dnorm(4, 10, 6) 
y <- 0.2 * dnorm(4, 0, 6)
x / (x + y)
```
  
  Using Bayes theorem, the probability that a company will issue a dividend this year based on a predictor value of X = 4 is approximately 0.72.  We arrived at this answer by multiplying the prior probability and the density function of the normal distribution for one class (i.e., "yes"), divided by that quantity plus the quantity using the prior and mean for the other class.  
  
  8) This question asks us to compare error rates for logistic regression and KNN.  In the case of a KNN where K=1, we're only considering 1 nearest neighbor.  Thus, in the training data, we'll have 0 errors, because we'll be able to perfectly predict each observation.  However, we know when we apply this overly flexible model to the test data, we'll tend to have high variance.  So, if we know the error rate averaged over both the training and test data is 18%, and we know the training error rate is 0%, then the test error rate must be 2 * 18 = 36%.  This is worse than the logistic regression classifier's test error rate of 30%.  
  
  9) This question is about odds.
  
  a) Recall, the formula for the odds (given one predictor) is:
  
$$\frac{p(X)}{1-p(X)} = \exp^{\beta_0 + \beta_1X}$$
  And the log odds or logit can be written as:
  
$$log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1X$$

  If we know the odds, we can re-write the first equation as:
  
$$\frac{p(X)}{1-p(X)} = 0.37$$

$$p(X) = \frac{0.37}{1+0.37}$$
$$p(X) = 0.27$$
  
  b) Now we're going to go the other way and calculate odds from the probability.
  
$$\frac{0.16}{1-0.16} = odds$$
$$odds = 0.19$$

10) 

11) 

12) This question has to do with logistic regression, softmax encoding, and the odds.

  a) The log odds of orange versus apple in the model is described by the following equation:
  
$$log(\frac{Pr(Y = \text{orange}|X = x)}{Pr(Y = \text{apple}|X = x)}) = \beta_0 + \beta_1X$$
  b) The log odds of range versus apple in my friend's model with the softmax encoding is given by:
  
$$log(\frac{Pr(Y = \text{orange}|X = x)}{Pr(Y = \text{apple}|X = x)}) = $$

### Applied

13) This question uses the `Weekly` data set containing 1,089 stock market returns from 1990 to 2010.

  a) Look at numerical and graphical summaries of the data and discuss any relevant patterns.
  
```{r applied 13a correlations}
weekly_long <- Weekly %>% 
  pivot_longer(
    cols = contains("Lag"), 
    names_to = "Lag", 
    values_to = "Value"
  )

weekly_long %>% 
  split(.$Lag) %>% 
  map_dbl(~cor(.x$Value, .x$Today))
```
  
The code above shows the correlations between each `Lag` variable and `Today`.  In general, the correlations are quite weak.  

```{r applied 13a plots}
p1 <- ggplot(Weekly, aes(Year, Volume)) + 
  geom_point() + 
  geom_smooth(se = FALSE)

p2 <- as.matrix(Weekly[, -9]) %>% 
  cor() %>% 
  ggcorrplot()

p3 <- Weekly %>% 
  dplyr::count(Year, Direction) %>% 
  group_by(Year) %>% 
  mutate(prop = n / sum(n)) %>% 
  ggplot(aes(Year, prop, fill = Direction)) + 
  geom_col(position = "stack") +
  geom_hline(yintercept = 0.5, linetype = "dashed", alpha = 0.5) 

wrap_plots(p1 + p2 / p3) + 
  plot_annotation(title = "Graphical summary of weekly stock returns from 1990 to 2010", 
                  tag_levels = "A", tag_suffix = ")")
```

From the summary plots above, we can see that in general, the volume of shares traded, or the average number of daily shares traded in billions, increases over the course of the data set from 1990 to 2010.  It's important to note that this relationship is non-linear, and the rate of increase in shares traded differs over time.  Additionally, in B) we can see that in general, there are relatively week correlations between variables, except for Year and Volume, which supports the overall trend in A).  Finally, plotting the proportion of up and down directions in the market over time, the proportions are relatively consistent.  We might also want to see if ther are differences in the correlations of variables over time, looking at each year independently.

```{r applied 13a corr plots}
weekly_split <- Weekly %>% 
  split(.$Year) %>% 
  map(~ .x %>% 
        dplyr::select(-c(Direction, Year)) %>% 
        as.matrix() %>% 
        cor())

names(weekly_split) <- unique(Weekly$Year)

map2(weekly_split, names(weekly_split), ~ ggcorrplot(.x, title = .y)) %>% 
  wrap_plots() + 
  plot_layout(guides = "collect")
```

The above correlation plots above are quite busy, but in general, we can see that over time, the correlation among the different variables changes.  

  b) Fit a logistic regression to model `Direction` against the 5 `Lag` variables with the full data set.
  
```{r applied 13b}
# Fit the model and print summary
weekly_logistic_full <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, 
  data = Weekly, 
  family = "binomial"
)

summary(weekly_logistic_full)
```
  
From the model output above, only the intercept and `Lag2` are statistically significant.

  c) Compute confusion matrix and overall fraction of correct predictions.
  
```{r applied 13c}
# Predictions
weekly_logistic_predictions <- predict(weekly_logistic_full, type = "response")
weekly_logistic_predictions <- ifelse(weekly_logistic_predictions > 0.5, "Up", "Down")

# Build confusion matrix
table(weekly_logistic_predictions, Weekly$Direction)

# Overall fraction of correct predictions
mean(weekly_logistic_predictions == Weekly$Direction) # 56% accuracy

# Same estimate as the following
(49 + 564) / 1089

# To get the error rate, 1 - accuracy
1 - (mean(weekly_logistic_predictions == Weekly$Direction)) # 44% error rate

# What types of mistakes is the logistic model making?
# Look at the number of true positives vs the number of true negatives
Weekly$predictions <- weekly_logistic_predictions

# Correct fraction of true positives
564 / (435 + 564) # 56% accurate in predicting true positives

# Correct fraction of true negatives
49 / (49 + 41) # 54% accurate in predicting true negatives

# False positive rate
round(435/1089, 3) # 0.399

# True positive rate
round(564/1089, 3) # 0.518

```
  
  The overall fraction of true predictions is 0.56, meaning the logistic regression model was accurate in predicting the direction of the stock market about 56% of the time.  We can calculate other diagnostic metrics to examine the types of mistakes made by the model.  First, we'll calculate the false positive rate, which is the number of false positives predicted divided by the total number of negatives (i.e., the total number of true outcomes that are negative).  Looking at the confusion matrix, the false positive rate is `r 435 / (49 + 435)`, meaning about 90% of the predicted positives are actually false positives.  Next, we can look at the true positive rate, which is the number of true positives, or the number of observations where both the prediction and true outcomes are positive, divided by the total number of known positive outcomes.  The true positive rate is `r 564 / (41 + 564)`.  This means that when the market goes up, the model correctly predicts this behavior about 93% of the time.  However, when the market goes down, the model only correctly predicts this behavior about `r round(49 / (435 + 49), 2) * 100`% of the time.  This links back perfectly with the false positive rate-- we calculated that about 90% of the time, the model will incorrectly predict that stock market will increase when it actually decreased.  
  
  d) Fit the logistic regression model using training data from 1990 to 2008 with only `Lag2` as a predictor, then compute the confusion matrix for the test data from 2009 to 2010.
  
```{r applied 13d}
# Create training and test sets
training_set <- filter(Weekly, !Year %in% c("2009", "2010"))
test_set <- filter(Weekly, Year %in% c("2009", "2010"))

# Fit logistic model
weekly_logistic_mod2 <- glm(
  Direction ~ Lag2, 
  data = training_set, 
  family = "binomial"
)

# Predictions with the test set
logistic_mod2_predictions <- predict(
  weekly_logistic_mod2, 
  newdata = test_set, 
  type = "response"
)

logistic_mod2_predictions <- ifelse(logistic_mod2_predictions > 0.5, "Up", "Down")

# Confusion matrix
mean(logistic_mod2_predictions == test_set$Direction) # 62.5% accuracy
table(logistic_mod2_predictions, test_set$Direction)
```

Overall, the model with just the `Lag2` variable using data from 1990 to 2008 accurately predicted the direction of the market about 63% of the time (accuracy = 0.625).  This indicates our test error rate is approximately 37%.  If we look closer at the errors, the fraction of false positives is `r 34 / (9 + 34)`, meaning about 80% of the errors made by the model are false positives.  In other words, when the model makes an error, about 80% of the time, it's predicted the stock market went up when in reality, the market went down.  The true positive rate is `r 56 / (56 + 5)`, meaning that when the market goes up, the model correctly predicts this outcome about 92% of the time (this is complimentary to the false negative rate, which is about 8%); when the market goes down, the proportion of correct predictions is `r 9 / (34 + 9)`, or about 20% of the time (this is complimentary to the false positive rate).  

  e) Repeat the same model above using LDA.
  
```{r applied 13e}
# Fit LDA and calculate predictions
lda_mod <- lda(Direction ~ Lag2, data = training_set)
lda_predictions <- predict(lda_mod, newdata = test_set)

# Accuracy
mean(lda_predictions$class == test_set$Direction) # 62.5% accurate

# Confusion matrix
table(lda_predictions$class, test_set$Direction)
```
  
The results of the LDA model are exactly the same as the logistic regression model- same accuracy, same number of false positives and false negatives.

  f) Repeat the same model above using QDA.
  
```{r applied 13f}
# Fit QDA and calculate predictions
qda_mod <- qda(Direction ~ Lag2, data = training_set)
qda_predictions <- predict(qda_mod, newdata = test_set)

# Accuracy
round(mean(qda_predictions$class == test_set$Direction), 3) # 0.587, slightly worse

# Confusion matrix
table(qda_predictions$class, test_set$Direction)
```

The accuracy on the test data with the QDA model is slightly worse than with the logistic regression and LDA models-- about 58% for the former and 63% for the latter.  when we look at the confusion matrix, we can see that when the true outcome of the market is up, the model makes correct predicts 100% of the time.  However, when the market goes down, the market never makes a correct prediction.  In summary, the QDA model predicts "Up" for all observations.

  g) Repeat the same model above using KNN with K=1.
  
```{r applied 13g}
# Prepare input data
training_mat <- as.matrix(training_set$Lag2)
test_mat <- as.matrix(test_set$Lag2)
training_labels <- training_set$Direction

# Classify
set.seed(1)
knn_mod <- knn(
  train = training_mat, 
  test = test_mat, 
  cl = training_labels, 
  k = 1
)

# Accuracy
mean(knn_mod == test_set$Direction) # 50% accurate

# Confusion matrix
table(knn_mod, test_set$Direction)
```
  
The KNN model with K=1 is only accurate 50% of the time.  When the market goes up, the model predicts the correct outcome only about 50.8% of the time; when the market goes down, the model predicts the correct outcome only about 49% of the time.  

  h) Repeat the same model above using naive Bayes.
  
```{r applied 13g}
# Fit model and make predictions
naive_bayes_mod <- naiveBayes(Direction ~ Lag2, data = training_set)
naive_bayes_predictions <- predict(naive_bayes_mod, newdata = test_set)

# Accuracy
mean(naive_bayes_predictions == test_set$Direction) # 58%, similar to QDA

# Confusion matrix
table(naive_bayes_predictions, test_set$Direction)
```
  
The naive Bayes model performs exactly the same as the QDA model.

  i) The logistic regression and LDA models provide the best results on these data.  
  
  j) Experiment with different combinations of predictors, including transformations and interactions.
  
```{r applied 13j}
# First fit a logistic regression model with all variables and interactions
logistic_all_terms <- glm(
  Direction ~ (Lag1 + Lag2 + Lag3 + Lag4 + Lag5)^2,
  data = training_set, 
  family = "binomial"
)

(coefs <- summary(logistic_all_terms) %>% 
  pluck("coefficients") %>% 
  data.frame() %>% 
  rownames_to_column("term") %>% 
  set_names("term", "estimate", "std_error", "z_value", "pval"))

# None are significant, but Lag2 and Lag3:Lag4 have the lowest p-values
# Build a new model with these terms only
logistic_subset <- glm(
  Direction ~ Lag2 + Lag3*Lag4,
  data = training_set, 
  family = "binomial"
)

summary(logistic_subset)

# Lag2 is significant, but none of the others are-- let's see how this performs on the test data anyway
logistic_subset_predictions <- predict(
  logistic_subset, 
  newdata = test_set
)

# Accuracy
mean(logistic_subset_predictions == test_set$Direction) # 0

# Now try fitting a KNN model with all Lag variables 
training_mat <- dplyr::select(training_set, contains("Lag")) %>% 
  as.matrix()

test_mat <- dplyr::select(test_set, contains("Lag")) %>% 
  as.matrix()

# Iterate over different values for K from 1-20
set.seed(1)
knn_results <- map(1:20, ~ class::knn(
  train = training_mat, 
  test = test_mat, 
  cl = training_labels, 
  k = .x)
)

names(knn_results) <- paste0("k_", 1:20)

knn_accuracy <- data.frame(
  k = seq(1, 20), 
  accuracy = map_dbl(knn_results, ~ mean(.x == test_set$Direction))
)

# K= 8 is most accurate
ggplot(knn_accuracy, aes(k, accuracy)) + 
  geom_point() + 
  geom_line()

# Confusion matrix
table(knn_results$k_8, test_set$Direction)

# False positive rate
25 / (18 + 25) # 0.58

# False negative rate
18 / (43 + 18) # 0.29
```
  
Looking at the logistic model, there are no statistically significant interaction terms to include in the model (although we haven't tried higher order polynomials).  We also tried fitting the KNN to the data with all lag variables included, but the accuracy is not substantially better than 50%.  The highest accuracy achieved was with K=8, but still it predicted the correct outcome only about 60% of the time.  

14) This problem uses the `Auto` data set to predict whether a car gets high or low gas mileage.  

  a) If `mpg` is greater than the median, recode as 1; otherwise, recode as 0.
  
```{r applied 14a}
# Create binary variable for Auto data
Auto <- mutate(Auto, mpg01 = as.factor(ifelse(mpg > median(mpg), 1, 0)))
```
  
  b) Exploratory data analysis between `mpg01` and other variables.

```{r applied 14b}
p1 <- ggplot(Auto, aes(horsepower, displacement, color = mpg01)) + 
  geom_jitter() + 
  facet_wrap(~mpg01) +
  gghighlight::gghighlight() + 
  labs(x = "Horsepower", y = "Engine displacement")

p2 <- ggplot(Auto, aes(mpg01, weight)) + 
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(aes(color = mpg01)) + 
  labs(x = "Gas mileage", y = "Weight (lbs)") + 
  theme(legend.position = "none")

p3 <- Auto %>% 
  group_by(origin) %>% 
  summarise(prop_high = length(which(mpg01 == 1)) / n(), 
            prop_low = 1-prop_high) %>% 
  pivot_longer(-origin, names_to = "prop", values_to = "value") %>% 
  mutate(prop = factor(prop, levels = c("prop_low", "prop_high"))) %>% 
  ggplot(aes(origin, value, fill = prop)) + 
  geom_col(position = "stack") + 
  labs(x = "Origin") + 
  theme(legend.position = "none")

p4 <- Auto %>% 
  count(origin, mpg01) %>% 
  ggplot(aes(origin, n, fill = mpg01)) + 
  geom_col(position = "dodge") + 
  labs(x = "Origin") + 
  theme(legend.position = "none")

p1 / (p2 + p3 + p4)
```

Looking at a few exploratory data analysis plots, there appears to be an association between horsepower, engine displacement, and gas mileage.  In general, vehicles with lower horsepower and engine displacement have high gas mileage.  Not surprisingly, lighter vehicles also tend to have high gas mileage.  There also appears to be an association between the year the vehicle was manufactured and gas mileage, with cars manufactured more recently having high gas mileage relative to older ones.  Finally, when looking at the origin of the vehicle, European and Japanese manufacturers produce higher proportions of high gas mileage vehicles relative to those produced in the United States.  

  c) Split the data into a training set and a test set.  There are only 392 observations, so I'll randomly sample about 80% of the data (313) and use the remaining subset to test the accuracy of the classifier.
  
```{r applied 14c}
Auto$id <- rownames(Auto)

set.seed(1)
training_ids <- sample(Auto$id, 313)

# Split
training_auto <- filter(Auto, id %in% training_ids)
test_auto <- filter(Auto, !id %in% training_ids)

# Fit a logistic regression model to see which variables may be associated with mpg01
logistic_auto <- glm(
  mpg01 ~ cylinders + displacement + horsepower + weight + acceleration + year + origin,
  data = training_auto,
  family = "binomial"
)

summary(logistic_auto)
```
  
Looking at the results of the logistic regression model with all variables, only the weight and year are statistically significant, so we'll use these in subsequent models.

  d) LDA on training data.
  
```{r applied 14d}
# Fit model
lda_auto <- lda(
  mpg01 ~ weight + year, 
  data = training_auto
)

# Predictions
lda_predictions <- predict(lda_auto, newdata = test_auto)

# Test error
1 - mean(lda_predictions$class == test_auto$mpg01) # about 7.5%, very low!
```

  e) QDA on the training data.
  
```{r applied 14e}
# Fit model and make predictions
qda_auto <- qda(
  mpg01 ~ weight + year, 
  data = training_auto
)

qda_predictions <- predict(qda_auto, newdata = test_auto)
1 - mean(qda_predictions$class == test_auto$mpg01) # about 10%, a bit higher but pretty good
```


  f) Logistic regression.
  
```{r applied 14f}
logistic_auto <- glm(
  mpg01 ~ weight + year, 
  data = training_auto, 
  family = "binomial"
)

logistic_predictions <- predict(logistic_auto, newdata = test_auto, type = "response")
logistic_predictions <- ifelse(logistic_predictions < 0.5, 0, 1)
1 - mean(logistic_predictions == test_auto$mpg01) # about 8.8%, better than QDA but not quite as good as LDA
```

  g) Naive Bayes.
  
```{r applied 14g}
naive_bayes_auto <- e1071::naiveBayes(
  mpg01 ~ weight + year,
  data = training_auto
)

(naive_bayes_predictions <- predict(naive_bayes_auto, newdata = test_auto))
1 - mean(naive_bayes_predictions == test_auto$mpg01) # about 10%, very similar to the QDA model
```

  h) KNN with several values of K.
  
```{r applied 14h}
training_mat <- as.matrix(dplyr::select(training_auto, weight, year))
test_mat <- as.matrix(dplyr::select(test_auto, weight, year))
training_labels <- training_auto$mpg01

set.seed(1)
knn_auto <- map(1:20, ~ class::knn(
  train = training_mat, 
  test = test_mat,
  cl = training_labels)
)

knn_res <- data.frame(
  k = 1:20,
  test_error = map_dbl(knn_auto, ~ 1 - mean(.x == test_auto$mpg01))
)

arrange(knn_res, test_error) # K=3 is the lowest with 10%, but most have either 10 or 11% error rates
```


