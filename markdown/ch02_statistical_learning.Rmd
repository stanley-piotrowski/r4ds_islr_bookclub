---
title: 'Chapter 02: Statistical Learning'
author: "Stan Piotrowski"
date: "`r format(Sys.Date(), '%B %d %Y')`"
output:
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Notes

### Prediction and inference 

* If we have a quantitative response $Y$ and a set of $p$ predictors, we can write a general form of the relationship as $Y = f(X) + \epsilon$
* The error term ($\epsilon$) is a random error term, also referred to as irreducible error, and has mean zero.
* In most situations, the true functional relationship between $Y$ and a set of $p$ predictors is not known, in which case we are estimating the function $f$ in the prediction setting (i.e., where we don't know the output).
* In the prediction setting, we re-write the equation as $\hat{Y} = \hat{f}(X)$, and the process of estimating the functional relationship induces additional error, called the reducible error.
* In an inference setting, we aren't necessarily concerned with predicting a response, but understanding the association between $Y$ and the set of $p$ predictors.
* When we are interested in understanding the association between $Y$ and $p$ predictors, we are looking at identifying the exact functional form of the relationship.

### Parametric vs. non-parametric methods

* The most important distinction between parametric and non-parametric methods is the former makes explicit assumptions about the shape of the function $f$, while the latter does not.
* An example of a parametric method is ordinary least squares regression, which assumes that the relationship between $Y$ and a set of $p$ predictors is linear and is solely explained by the set of $p$ predictors.
* In contrast, an example of a non-parametric method is a thin-plate spline used for interpolation and smoothing, which instead instead of assuming a functional form for $f$, tries to develop an estimate that is as close as possible to the observed data.

### Prediction accuracy and model interpretability trade-off

* When choosing a statistical learning method, there is a trade-off between flexibility and interpretability: flexible methods are generally better for prediction accuracy, but are less interpretable, while inflexible methods have relatively poor prediction accuracy but are relatively easier to interpret.
* For example, if we are interested in inference, inflexible models that reduce the problem to a simplified version of reality with just a few predictors is more desirable: the model performs relatively well on training data and easily generalizes to unseen test data.
* Further, inflexible methods allow us to generalize and understand the nature of the relationship (i.e., interpret the model) between the response and predictors.
* On the other hand, if we are interested in prediction, flexible models can capture further intricaces in the data, but suffer from overfitting (essentially capturing noise in the data rather than the true functional relationship).

### Unsupervised vs. supervised learning methods

* In supervised settings, we are training a learning method using a set of predictors and known outcomes, then applying the model to unseen test data and essentially comparing how often the model identifies the true known response.
* In an unsupervised setting, we don't have outputs; rather, we are interested in discovering meaningful patterns just using the data.
* An example of supervised learning is the ordinary least squares regression; clustering using principal components analysis is an example of unsupervised learning.

### Regression vs. classification problems

* In general, quantitative responses can be thought of as regression problems (although there are exceptions like logistic regression for categorical data), while qualitative responses can be thought of as classification problems.

### Evaluating model accuracy

* The most important rule to remember is that there is no single "best" method for all data sets and careful exploration and interpretation is needed to select a model which balances the trade-off between flexibility and interpretability.
* The mean squared error (MSE) is a metric used in the regression setting to quantify how close the predicted response is from the observed response for a particular value of predictor.  
* In essence, MSE is the mean of the squared differences between the predicted value and the observed response value.
* Interpreting the MSE is simple: the smaller the MSE, the closer the model predicted values are to the observed values.
* Importantly, simply because a given model out of a set has the lowest training MSE doesn't mean that it will have the lowest test MSE- this highlights the importance in balancing flexibility and the ability of the model to generalize to unseen test data.
* Degrees of freedom in statistical learning is the quantity that describes the flexibility of a curve to data: more flexible methods have higher degrees of freedom and generally fit the data more closely, but at the expense of potentially performing poorly on test data (generalization problem).
* When evaluating model flexibility and accuracy, as the flexibility of the method increases, the training MSE will consistently decrease, while the test MSE will exhibit a U-shaped curve: it will decrease up to a certain inflection point, beyond which it increases rapidly because of the generalization problem.
* We can decompose the expected test MSE using the following equation: 

$E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon)$

* The first term, $E(y_0 - \hat{f}(x_0))^2$, describes the expected test MSE, or the mean test MSE if we were to calculate the test MSE over successive training data sets.
* The second term, $Var(\hat{f}(x_0))$, describes the variance in the functional form of $f$: essentially, how much does $f$ change over successive training data sets.
* The third term, $[Bias(\hat{f}(x_0))]^2$, is the absolute value of the squared bias, or the error introduced by approximating reality with a simplified model.
* Finally, the last term, $Var(\epsilon)$, is the variance in the irreducible error term over successive training data sets.
* In general, using increasingly flexible methods will lead to an increase in $Var(\hat{f}(x_0))$ and a decrease in $[Bias(\hat{f}(x_0))]^2$; the opposite is true for inflexible models.

### Classification

* The Bayes classifier is considered the gold-standard and evaluates the conditional probability of a response belonging to a specific class given a value of the predictor variable: the assigned class is whichever has a probability > 0.5.
* However, using the Bayes classifier requires that we know the conditional distribution of $Y$ given $X$, which is generally unknown.
* A close approximation is the $K$-nearest neighbors (KNN) classifier, which looks at each test observation in training data and assigns a conditional probability based on the fraction of $K$ points that are closest to it; the assigned class is the one with the highest estimated conditional probability.
* In general, higher numbers of $K$ using the KNN classifier decreases flexibility and the variance in functional form at the expense of increasing bias.

## Exercises

### Conceptual

1) This exercises poses a question regarding the performance of flexible statistical learning methods relative to inflexible methods.

    a) For each of these scenarios, we need to consider the variance of the function describing the relationship between the response and the predictor(s) between successive training sets and the bias, or the error introduced when models are used to simplify reality.  Both of these metrics factor into the overall test mean squared error (MSE), or the mean squared difference between the predicted value or the value estimated by the function and the observed value (in the case of supervised statistical learning).   
    
    b) In a case where there is a small sample size and a large number of predictors, an inflexible model would perform better than a flexible model.  Although the bias would be larger using the inflexible model, the power of this approach is the ability to be able to identify the predictors that are associated with the response, assuming that only a small fraction are actually important.
    
    c) In a scenario where the relationship between the predictors and the response is highly non-linear, we would expect the flexible statistical learning method to be better than an inflexible method (generally).  Inflexible learning methods, like ordinary least squares regression, for example, will likely have a low variance between successive training data sets, but have a large bias because we are attempting to explain a non-linear relationship with assumptions in a simplified linear reality. 
    
    d) In contrast to c), in a scenario where the variance of the error terms is extremely high, an inflexible method may be preferred because fitting a flexible statistical model on successive training data sets that are substantially different could be driving the high variance.  

2) The following questions ask to explain whether the scenario is a classification or regression problem, and whether the primary question of interest is inference or prediction.  We are also asked to identify $n$, the number of samples, and $p$, the predictors.
  
    a) In this scenario, we are dealing with $n=500$ samples (i.e., the 500 firms) and $p=3$ predictors: profit, number of employees, and industry.  In this example, the CEO salary is the response.  This is a classic regression problem where the primary goal is inference, because we are interested in understanding the relationship between the set of predictors and CEO salary and the data generating mechanism.  For example, we would expect profit to have a positive influence on CEO salary, but what about number of employees and industry?  Further, we are not necessarily interested in predicting a CEO's salary based on historic data.
    
    b) This is a classic example of a classification problem where the primary goal is prediction using historical data  We have $n=20$ products and $p=13$ predictors.  Here, the response is whether the product was a success or failure, based on these predictors.  I argue that the goal is prediction here because when we're launching a new product, we want to know how likely it is to succeed, plain and simple, and the prompt does not suggest that we are interested in understanding _how_ exactly each of these variables influence the response.
    
    It seems plausible to pivot and frame this as an inference problem as well, as presumably you would want to know which variables were most influential in whether the product was a success or failure.  
    
    c) This is another example where the goal is prediction, but framed as a regression problem because we are interested in predicting a quantitative response.  Since we have weekly data for all of 2012, we have $n=52$ samples, with $p=3$ predictors.  
    
3) This question revisits the bias-variance decomposition and asks us to draw a sketch of a typical plot showing the bias, variance, training and test error, and Bayes or irreducible error curves.  Instead, I'll explain how each curve would look, generally, and why they have the shapes they do.

    On the x-axis we would display the flexibility of the model, going from less flexible (less) to more flexible (right).  In general, less flexible models will have the following properties:

    * __Bias:__ less flexible methods suffer from the fact that they attempt to approximate complex reality using a simplified model, which inherently introduces error due to differences between the observed data and the responses predicted by the estimated function.
    
    * __Variance:__ less flexible methods generally have lower variance because the models may assume the relationship is linear (in the case of linear regression) and go through "most" of the data without trying to go through each point.
    
     * __training error:__ higher training error, again getting back to the fact that the model attempts to simplify the relationship, which introduces error if the relationship is not linear, for example.
     
     * __test error:__ lower training error, because the model is able to generalize well to unseen test data.
     
     * __irreducible error:__ this stays fixed, because it is not related to the functional form of the relationship betweeen the response and predictors.
     
     On the other hand, more flexible models will generally show the opposite of each of the properties described above.
     
4) This question asks us to think of applications for statistical learning methods.

    a) Classification applications:
    
        i) In population genetics, we are often interested in exploring how genetic variation across the genome can be used for understanding population structure.  A classification problem focused on inference would be building a statistical learning method which classifies samples into groups or "populations" based on the genetic data alone.
        
        ii) Framing the above example slightly differently, we have some genotype data for fish spawning in different rivers and we know fish A only spawns in river A, and so on and so forth.  We are then given several fish of unknown origin and asked to identify the population of origin using the genotype data alone.  Here, we're presented with a classification problem where the primary goal is prediction to put the fish into the correct spawning river.
        
        iii) In a different example, we might be interested in knowing which variables affect whether a person will default on a loan.  Here, the question is one of inference, because we are interested in the functional form of the relationship between the binary outcome and the set of predictors.
    
    b) Regression applications:
    
        i) Let's say we are interested in understanding how a series of predictors influences the price of a home in a given city.  Here, the predictors may be the lot size, school system rating in the area, square footage of the home, number of bathrooms and bedrooms, etc., and the response is the price of the home.  This would be an inference question, as we're interested in the relationship between the predictors and the response.
      
        ii) In another scenario, we may be interested in using regression to predict the highway fuel economy of a vehicle (the response) given a set of predictors like the model, the engine displacement, number of cylinders, transmission, and the fuel type.  This is framed as a prediction question because we don't really care about the relationship; we're just interested in predicting a quantitative response correctly as often as possible.
      
        iii) Framing the above question slightly different as an inference problem, I could could pose a question as an auto-maker: given all of these predictor variables, which subset has the most influence on the highway fuel economy.  That could help direct effects towards improving those variables which have a positive influence, and trying to mitigate for the variables that have a negative influence.
    
    c) Cluster analysis:
    
        i) In a scenario where I had RNA-seq data generated from many different cell types in a piece of tissue, cluster analysis would facilitate the discovery of clusters using the data alone which may correspond to cell type.
      
        ii) In another situation suppose I sampled fish in a lake and was interested in characterizing population structure using genetic data alone: this would be a great application for cluster analysis to reveal any interesting patterns in the data.
      
        iii) Finally, in a different scenario (although not a very plausible one), say we collected data on vehicles including engine displacement, number of cylinders, transmission, drive train, and fuel economy.  We could use cluster analysis to identify interesting groups within those data, which may correspond to vehicle class (e.g., compact, etc).
      
5) The advantages of using a very flexible model is that you don't have to explicitly assume the data follow a defined distribution (e.g., linear).  This can be beneficial in scenarios where you aren't concerned with interpretability; rather, your sole focus may be prediction accuracy.  On the other hand, very flexible models suffer from a general lack of interpretability, overfitting, and lack of generalizability, particularly as flexibility increas  In some situations, a less flexible model may be preferred if the goal is inference and interpreting model coefficients with the added benefit of generally lower test mean squared error relative to more flexible models.

6) In essence, a parametric statistical learning method simplifies a problem by boiling down the shape of the function $f$ down to just a few parameters and assumes that shape follows a known distribution.  In a regression setting, parametric learning methods are advantageous in that increase interpretability and in general, simpler models are often preferred relative to overly complex models.  Additionally, parametric methods can be used in situations with smaller sample sizes (generally far more samples are needed when using non-parametric methods).  In contrast to parametric methods, non-parametric methods don't rely on explicit assumptions about the shape of the function $f$, so are much more flexible, but at the expense of interpretability: non-parametric methods aren't simplifying a functional form down to just a few parameters.  Further, more flexible methods suffer from overfitting and high test error associated with poor performance on unseen test data.

7) The table below is re-recreated from the textbook:

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(kableExtra)

df <- tribble(
  ~X1, ~X2, ~X3, ~Y,
  0, 3, 0, "Red", 
  2, 0, 0, "Red", 
  0, 1, 3, "Red", 
  0, 1, 2, "Green", 
  -1, 0, 1, "Green", 
  1, 1, 1, "Red"
) %>% 
  rownames_to_column("Obs.")

df %>% 
  kbl() %>% 
  kable_classic_2(full_width = FALSE)
```

a) First, we'll calculate the Euclidean distance between each observation as a test point with the values $X1 = X2 = X3 = 0$.  Note, the Euclidean distance formula is $\sqrt(\sum((x_i - y_i)^2))$.  We'll sort the table below based on the calculated Euclidean distance.
    
```{r warning = FALSE, message = FALSE}
# Calculate Euclidean distance
df %>% 
  pivot_longer(-c(`Obs.`, Y), names_to = "X", values_to = "Value") %>% 
  mutate(Distance = (Value - 0)^2) %>% 
  group_by(`Obs.`, Y) %>% 
  summarise(Distance = round(sqrt(sum(Distance)), 2)) %>% 
  arrange(Distance) %>% 
  kbl() %>% 
  kable_classic_2(full_width = FALSE)
```
    
b) For each test point, the KNN method looks at the $K$ number of points closest to the test point using the Euclidean distance and estimates the probabilities of belonging to one of $K$ classes.  If we chose a highly flexible model of $K=1$, we'll assign the test point to the class ("Red" or "Green") of the closest point (i.e., the point with the smallest Euclidean distance).  In this case, the point with the smallest Euclidean distance 1.41 is "Green."
    
    We can check this manual calculation using the `knn()` function from the `class` package.  The function requires us to define a training set, a test set, the classes of the training set, and the number of neighbors to be considered in the KNN algorithm.  We'll use the first table as the training data, the test point values for the single test case, and define the number of neighbors.
    
```{r}
# KNN with K=1
library(class)

training_set <- df %>% 
  dplyr::select(-c(`Obs.`, Y))
test_set <- c(X1 = 0, X2 = 0, X3 = 0)
classes <- df %>% 
  dplyr::select(Y)

# Run the KNN classifier
knn(train = training_set,
    test = test_set,
    cl = classes$Y, 
    k = 1)
```
    
c) If we modeled $K=3$, looking now at the 3 closest points, we see that the most commonly occuring class (2/3) is "Red", so we'd assign the test point as "Red."  We can use the same approach as we did above to check our manual calculations.
    
```{r echo = TRUE}
# Run KNN with K=3
knn(train = training_set, 
    test = test_set,
    cl = classes$Y, 
    k = 3)
```
    
d) If the Bayes decision boundary in this particular problem is highly non-linear, the "best" value for $K$ would be small.  When the value for $K$ is small, for each individiual test point we are only evaluating the closest neighbor to assign a class, which will allow the KNN decision boundary to be extremely flexible and essentially "custom catered" for each test point.  When $K$ is large, we are considering more neighbors for each individual test point to estimate a probability and assign a class, which generally forces the KNN decision boundary to follow a linear form.
    
### Applied
